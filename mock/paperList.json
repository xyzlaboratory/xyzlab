[
  {
    "id": 0,
    "title": "Pose-Based Composition Improvementfor Portrait Photographs",
    "picture": "img/paper_img/0.png",
    "brief": "<p>This paper studies the composition in portrait paintings and develops an algorithm to improve the composition of portrait photographs based on example portrait paintings.</p >",
    "content": "<p>This paper studies the composition in portrait paintings and develops an algorithm to improve the composition of portrait photographs based on example portrait paintings.A study of portrait paintings shows that the placement of the face and the figure is pose-related. Based on this observation,this paper develops an algorithm to improve the composition of a portrait photograph by learning the placement of the face and the figure from an example portrait painting. This example portrait painting is selected based on the similarity of its figure pose to that of the input photograph. This similarity measure is modeled as a graph matching problem. Finally,space cropping is performed using an optimization function to assign a similar location for each body part of the figure inthe photograph with that of the figure in the example portrait painting. The experimental results demonstrate the effectiveness of the proposed method. A user study shows that the proposed pose-based composition improvement is preferred more than rule-based methods and learning-based methods.</p >",
    "date": "MARCH 1, 2017",
    "author": "Xiaoyan Zhang, Member, IEEE, Zhuopeng Li, Martin Constable,Kap Luk Chan, Member, IEEE, Zhenhua Tang, and Gaoyang Tang",
    "category": "IEEE Transactions on Circuits and Systems for Video Technology",
    "github": "none",
    "pdf": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7952499"
  },
  {
    "id": 1,
    "title": "Haze removal method for natural restoration of images with sky",
    "picture": "img/paper_img/1.png",
    "brief": "<p>Most haze removal methods fail to restore long-shot images naturally, especially for the sky region. </p >",
    "content": "<p>Most haze removal methods fail to restore long-shot images naturally, especially for the sky region. To solve this problem, we proposed a Fusion of Luminance and Dark Channel Prior (F-LDCP) method to effectively restore long-shot images with sky. The transmission values estimated based on a luminance model and dark channel prior model are fused together based on a soft segmentation. The transmission estimated from the luminance model mainly contributes to the sky region, while that from the dark channel prior for the foreground region. The airlight also is adjusted to adapt to real light by sky region detection. A user study and objective assessment comparison with a variety of methods on long-shot haze images demonstrate that our method retains visual truth and removes the haze effectively</p >",
    "date": "September 7, 2017",
    "author": "Yingying Zhu, Gaoyang Tang, Xiaoyan Zhang, Jianmin Jiang, Qi Tian",
    "category": "Neurocomputing",
    "github": "https://github.com/tygrer/F-LDCP",
    "pdf": "https://www.sciencedirect.com/science/article/pii/S0925231217314856"
  },
  {
    "id": 2,
    "title": "An eFTD-VP framework for efficiently generating patient-specific anatomically detailed facial soft tissue FE mesh for craniomaxillofacial surgery simulation",
    "picture": "img/paper_img/2.png",
    "brief": "<p>Accurate surgical planning and prediction of craniomaxillofacial surgery outcome requires simulation of soft tissue changes following osteotomy.</p >",
    "content": "<p>Accurate surgical planning and prediction of craniomaxillofacial surgery outcome requires simulation of soft tissue changes following osteotomy. This can only be achieved by using an anatomically detailed facial soft tissue model. The current state-of-the-art of model generation is not appropriate to clinical applications due to the time-intensive nature of manual segmentation and volumetric mesh generation. The conventional patient-specific finite element (FE) mesh generation methods are to deform a template FE mesh to match the shape of a patient based on registration. However, these methods commonly produce element distortion. Additionally, the mesh density for patients depends on that of the template model. It could not be adjusted to conduct mesh density sensitivity analysis. In this study, we propose a new framework of patient-specific facial soft tissue FE mesh generation. The goal of the developed method is to efficiently generate a high-quality patient-specific hexahedral FE mesh with adjustable mesh density while preserving the accuracy in anatomical structure correspondence. Our FE mesh is generated by eFace template deformation followed by volumetric parametrization. First, the patient-specific anatomically detailed facial soft tissue model (including skin, mucosa, and muscles) is generated by deforming an eFace template model. The adaptation of the eFace template model is achieved by using a hybrid landmark-based morphing and dense surface fitting approach followed by a thin-plate spline interpolation. Then, high-quality hexahedral mesh is constructed by using volumetric parameterization. The user can control the resolution of hexahedron mesh to best reflect clinicians’ need. Our approach was validated using 30 patient models and 4 visible human datasets. The generated patient-specific FE mesh showed high surface matching accuracy, element quality, and internal structure matching accuracy. They can be directly and effectively used for clinical simulation of facial soft tissue change.</p >",
    "date": "October 12, 2017",
    "author": "X Zhang，D Kim，S Shen，P Yuan，S Liu，Z Tang，G Zhang，X Zhou，J Gateno，Liebschner, Michael A. K",
    "category": "Biomechanics & Modeling in Mechanobiology",
    "github": "none",
    "pdf": "https://link.springer.com/content/pdf/10.1007/s10237-017-0967-6.pdf"
  },
  {
    "id": 3,
    "title": "Transfer of content-aware vignetting effect from paintings to photographs",
    "picture": "img/paper_img/3.jpg",
    "brief": "<p>This paper discusses how the vignetting effect of paintings may be transferred to photographs, with attention to center-corner contrast. First, the lightness distribution of both are analyzed. </p >",
    "content": "<p>This paper discusses how the vignetting effect of paintings may be transferred to photographs, with attention to center-corner contrast. First, the lightness distribution of both are analyzed. The results show that the painter’s vignette is more complex than that achieved using common digital post-processing methods. It is shown to involve both the 2D and 3D geometry of the scene. Then, an algorithm is developed to transfer the vignetting effect from an example painting to a photograph. The example painting is selected as that has similar contextual geometry with the photograph. The lightness weighting pattern extracted from the selected example painting is adaptively blended with the input photograph to create vignetting effect. In order to avoid over-brightened or over-darkened regions in the enhancement result, the extracted lightness weighting pattern is corrected using a nonlinear curve. A content-aware interpolation method is also proposed to warp the lightness weighting to fit the contextual structure of the photograph. Finally, the local contrast is restored. Experiments show that the proposed algorithm can successfully perform this function. The resulting vignetting effect is more naturally presented with regard to esthetic composition as compared with vignetting achieved with popular software tools and camera models.</p >",
    "date": " February 3 ,2018",
    "author": "X Zhang，M Constable，KL Chan",
    "category": "Multimedia Tools and Applications",
    "github": "none",
    "pdf": "https://link.springer.com/content/pdf/10.1007%2Fs11042-018-5629-x.pdf"
  },
  {
    "id": 4,
    "title": "An Articulated Structure-aware Network for 3D HumanPose Estimation",
    "picture": "img/paper_img/4.png",
    "brief": "<p>In this paper, we propose a new end-to-end articulated structure-aware network to regress 3D joint coordinates from the given 2D joint detections.</p >",
    "content": "<p>In this paper, we propose a new end-to-end articulated structure-aware network to regress 3D joint coordinates from the given 2D joint detections. The proposed method is capable of dealing with hard joints well that usually fail existing methods. Specifically, our framework cascades a refinement network with a basic network for two types of joints, and employs a attention module to simulate a camera projection model. In addition, we propose to use a random enhancement module to intensify the constraints between joints. Experimental results on the Human3.6M and HumanEva databases demonstrate the effectiveness and flexibility of the proposed network, and errors of hard joints and bone lengths are significantly reduced, compared with state-of-the-art approaches.</p >",
    "date": "2019",
    "author": "Zhenhua Tang, Xiaoyan Zhang, Junhui Hou",
    "category": "Proceedings of The Eleventh Asian Conference on Machine Learning",
    "github": "none",
    "pdf": "http://proceedings.mlr.press/v101/tang19a/tang19a.pdf"
  },
  {
    "id": 5,
    "title": "3D human pose estimation via human structure-aware fully connected network ",
    "picture": "img/paper_img/5.png",
    "brief": "<p>Existing 3D human pose estimation (3D-HPE) methods focus on reducing the overall joint error, resulting in endpoints and bone lengths with large errors. </p >",
    "content": "<p>Existing 3D human pose estimation (3D-HPE) methods focus on reducing the overall joint error, resulting in endpoints and bone lengths with large errors. To address this issue, we propose a human structure-aware network, which is capable of recovering 3D joint locations from given 2D joint detections. We cascade a refinement network with a basic network in a residual learning manner, meanwhile fuse the features from 2D and 3D coordinates by a residual connection. Specifically, our refinement network employs a dual-channel structure, in which the symmetrical endpoints are divided into two parts and refined separately. Such a structure is able to avoid the mutual interference of joints with large errors to promise reliable 3D features. Experimental results on the Human3.6M dataset demonstrate that our network reduces the errors of both endpoints and bone lengths compared with existing state-of-the-art approaches.</p >",
    "date": "May 30, 2019",
    "author": "X Zhang，Z Tang，J Hou，Y Hao",
    "category": "Pattern Recognition Letters",
    "github": "none",
    "pdf": "https://www.sciencedirect.com/science/article/pii/S016786551830432X"
  },
  {
    "id": 6,
    "title": "基于深度学习的髋关节应力分布算法研究",
    "picture": "img/paper_img/6.png",
    "brief": "<p>针对髋关节软骨的应力分布算法研究问题，设计了一个基于深度学习模型来代替有限元分析。</p >",
    "content": "<p>针对髋关节软骨的应力分布算法研究问题，设计了一个基于深度学习模型来代替有限元分析。该深度学习模型分为无监督学习模块和有监督学习模块，首先使用无监督学习模块对髋关节的软骨和股骨进行形状编码；之后实现对应力分布数据的编码与解码，使得应力数据能够与神经网络相结合；然后通过监督学习，利用编码好的应力数据进行监督，使神经网络学习得到一个从髋关节软骨和股骨的形状码到应力分布的应力码的映射关系；最终得到一个拟合的深度学习模型。此模型能够在一定程度上模拟有限元分析方法，但是由于其平均绝对误差和归一化平均绝对误差比较大，所以还不能完全替代有限元分析方法。在此基础上，进一步探索了新模型在特征利用上的局限，并提出了改进的方向。</p >",
    "date": "September, 2019",
    "author": "刘远平，宋昱锴，张小燕，刘贤强",
    "category": "智能科学与技术学报",
    "github": "none",
    "pdf": "http://www.infocomm-journal.com/znkx/EN/10.11959/j.issn.2096-6652.201934"
  },
  {
    "id": 7,
    "title": "Deep photographic style transfer guided by semantic correspondence",
    "picture": "img/paper_img/7.png",
    "brief": "<p>The objective of this paper is to develop an effective photographic style transfer method while preserving the semantic correspondence between the style and content images for both scenery and portrait images.</p >",
    "content": "<p>The objective of this paper is to develop an effective photographic style transfer method while preserving the semantic correspondence between the style and content images for both scenery and portrait images. A semantic correspondence guided photographic style transfer algorithm is developed, which is to ensure that the semantic structure of the content image has not been changed while the color of the style images is being migrated. The semantic correspondence is constructed in large scale regions based on image segmentation and also in local scale patches using Nearest-neighbor Field Search in the deep feature domain. Based on the semantic correspondence, a matting optimization is utilized to optimize the style transfer result to ensure the semantic accuracy and transfer faithfulness. The proposed style transfer method is further extended to automatically retrieve the style images from a database to make style transfer more-friendly. The experimental results show that our method could successfully conduct the style transfer while preserving semantic</p >",
    "date": " September 3 ,2019",
    "author": "Xiaoyan Zhang · Xiaole Zhang · Zhijiao Xiao",
    "category": "Multimedia Tools and Applications",
    "github": "none",
    "pdf": "https://link.springer.com/content/pdf/10.1007/s11042-019-08099-7.pdf"
  },
  {
    "id": 8,
    "title": "Emotion Attention-Aware Collaborative Deep Reinforcement Learning for Image Cropping",
    "picture": "img/paper_img/8.png",
    "brief": "<p>This paper proposes a collaborative deep reinforcement learning model for automatic image cropping (called CDRL-IC).</p >",
    "content": "<p>This paper proposes a collaborative deep reinforcement learning model for automatic image cropping (called CDRL-IC). By modeling image cropping as a decision-making process of reinforcement learning, our model could generateoptimal cropping result in a few moving and zooming steps.An image with good composition is a comprehensive result by considering the relative importance of objects and also the spatial organization of visual elements. Therefore, emotion attention information which indicates the relationship and importance between objects is applied together with contextual information of color image for image cropping. In order to sufficiently use the emotion attention map and the color image, they are processed by two collaborative agents. The two agents make their primary learning separately and then share information through an information interaction module for making joint action prediction. In order to efficiently evaluate the cropping quality in the reward function, weighted Intersection Over Union (WIoU) is designed by integrating emotion attention map in the traditional IoU. Our CDRL-IC model is tested on a variety of datasets for both image cropping and thumbnail generation. The experiments show that our CDRL-IC model outperforms state of-the-art methods on these benchmark datasets.</p >",
    "date": " NOVEMBER,2019",
    "author": "Xiaoyan Zhang, Zhuopeng Li, Jianmin Jiang",
    "category": "IEEE Transactions on Multimedia",
    "github": "none",
    "pdf": "http://ieeexplore.ieee.org/servlet/opac?punumber=6046l"
  },
  {
    "id": 9,
    "title": "A Modular Software System for 3D Measurement of Acetabulum in Arthritic Dysplasia Hips",
    "picture": "img/paper_img/9.png",
    "brief": "<p>In order to help orthopedists evaluate the morphological characteristics of the acetabulum of patients with osteoarthritis of the hip more efficiently and accurately to identify the type of acetabular deformities which benefits the personalized preoperative planning, a 3-dimensional (3D) acetabular morphologic parameters measurements software dedicated to the hip was developed.</p >",
    "content": "<p>In order to help orthopedists evaluate the morphological characteristics of the acetabulum of patients with osteoarthritis of the hip more efficiently and accurately to identify the type of acetabular deformities which benefits the personalized preoperative planning, a 3-dimensional (3D) acetabular morphologic parameters measurements software dedicated to the hip was developed. The system includes four modules: 1) Identify the anterior pelvic plane (APP) of the pelvis model; 2) Identify the circular rim of the acetabular wall; 3) Automatically measure the 3D morphological parameters of the dysplastic acetabulum; 4) Interactively measure the 3D morphological parameters of the dysplastic acetabulum. The automatic parameter measurement function of this software could fast and accurately measure the 3D morphological parameters of the dysplastic acetabulum. These automatically measured parameters were close to those measured manually with error generally less than 2mm, and their average measurement time was nearly 10 times faster than that using the Mimics 17.0 system. For patients with large osteophyte, the 3D morphological parameters of the dysplastic acetabulum could be efficiently measured using the interactive measurement function of this software with simple operations. This software was used to measure acetabular morphological parameters in 61 patients. Two types of dysplastic acetabula were identified by the thickness of the medial wall on the lower margin of the acetabulum Tb: type I was a thin acetabulum (35 cases, Tb \\leqslant 10.0 mm) and type II was a thick acetabulum (26 cases, mm). According to the result of the acetabular morphological characteristic analysis, it can be found that the thickness of the medial wall is an important morphological characteristic for the THA preoperative surgical planning, and the thickened medial wall could be a misleading factor for the suboptimal placement of the cup.</p >",
    "date": "November 1, 2019",
    "author": "C Chen，P Wu，L Zheng，Y Liu，Y Song，X Zhang",
    "category": "IEEE International Conference on Bioinformatics and Biomedicine",
    "github": "none",
    "pdf": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8982957"
  },
  {
    "id": 10,
    "title": "Multi-view visual Bayesian personalized ranking for restaurant recommendation",
    "picture": "img/paper_img/10.png",
    "brief": "<p>In recent recommendation systems, the image information of items is often used in conjunction with deep convolution network to directly learn the visual features of items. However, the existing approaches usually use only one image to represent an item. </p >",
    "content": "<p>In recent recommendation systems, the image information of items is often used in conjunction with deep convolution network to directly learn the visual features of items. However, the existing approaches usually use only one image to represent an item. These approaches are inadequate for an item with multi-view related images. For a restaurant, it has visual information of food, drink, environment, and so on. Each view of an item can be represented by multiple images. In this paper, we propose a new factorization model that combines multi-view visual information with the implicit feedback data for restaurant prediction and ranking. The visual features (visual information) of images are extracted by using a deep convolution network and are integrated into a collaborative filtering framework. In order to conduct personalized recommendation better, the multi-view visual features are fused through user related weights. User related weights reflect the personalized visual preference for restaurants and the weights are different and independent between users. We applied this model to make personalized recommendations for users on two real-world restaurant review datasets. Experimental results show that our model with multi-view visual information achieves better performance than models without or with only single-view visual information.</p >",
    "date": "13 April 2020",
    "author": "Xiaoyan Zhang, Haihua Luo, Bowei Chen & Guibing Guo ",
    "category": "Applied Intelligence",
    "github": "none",
    "pdf": "https://link.springer.com/content/pdf/10.1007/s10489-020-01703-6.pdf"
  },
  {
    "id": 11,
    "title": "SEMANTIC-AWARE VIDEO STYLE TRANSFER BASED ON TEMPORAL CONSISTENT SPARSE PATCH CONSTRAINT",
    "picture": "img/paper_img/11.png",
    "brief": "<p>This paper proposes a practical style transfer method to synthesize a temporally smooth video whose style information is semantically consistent with the reference video. </p >",
    "content": "<p>This paper proposes a practical style transfer method to synthesize a temporally smooth video whose style information is semantically consistent with the reference video. Due to the lack of paired videos for training, we extend the structure of CycleGAN with sparse patch and temporal constraints, including a new semantic patch loss and a novel temporal loss. Our approach’s key insights are: (1) the semantically paired sparse patches chosen from synthesized videos and reference frames would promote the semantic meaning of style transfer, the preservation of video content, and the smoothness of results by minimizing the discrepancies between these paired patches. (2) the forward and backward temporal consistency among neighbouring frames can reduce the discontinuity in the synthesized video. Extensive quantitative and qualitative experiments on various metrics demonstrate the superiority of our method over state-of-the-art strategies.</p >",
    "date": " 9 June, 2021",
    "author": "Yaxin Liu; Xiaoyan Zhang; Xiaogang Xu",
    "category": "IEEE International Conference on Multimedia and Expo",
    "github": "none",
    "pdf": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9428352"
  },
  {
    "id": 12,
    "title": "PR-RL: Portrait Relighting via Deep Reinforcement Learning",
    "picture": "img/paper_img/12.png",
    "brief": "<p>In this paper, we propose a portrait relighting method based on deep reinforcement learning (called PR-RL). </p >",
    "content": "<p>In this paper, we propose a portrait relighting method based on deep reinforcement learning (called PR-RL). Our PR-RL model could conduct portrait relighting by sequentially predicting local light editing strokes, and use strokes to conduct dodge and burn operations on the image lightness, simulating image editing by artists using brush strokes. Reinforcement learning with Deep Deterministic Policy Gradient is introduced to design our PR-RL model, defining the action (stroke parameters) in a continuous space, through which a reward can be designed to guide the agent to learn and relight a portrait image like an artist. To optimize the relighting effect, we further enable the reward to be location relevant and hence a coarse-to-fine strategy can be applied to select corresponding actions and maximize the performance of the proposed method. In comparison with the existing efforts, our proposed PR-RL method is locally effective, scale-invariant and interpretable. We apply the proposed method to tasks of portrait relighting based on both SH-lighting and reference images. The experiments show that our PR-RL method outperforms state-of-the-art methods in generating locally effective and interpretable high resolution relighting results for wild portrait images.</p >",
    "date": "14 July 2021",
    "author": "Xiaoyan Zhang; Yukai Song; Zhuopeng Li; Jianmin Jiang",
    "category": "IEEE Transactions on Circuits and Systems for Video Technology",
    "github": "none",
    "pdf": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9484089"
  }
]
